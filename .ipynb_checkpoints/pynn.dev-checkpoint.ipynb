{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import pyneuralnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer(object):\n",
    "    def __init__(self, num_prev, num_neurons, activation_function='sigmoid'):\n",
    "        '''\n",
    "        Initialize weights randomly with He scaling.\n",
    "        \n",
    "        self.A -- [num_neurons, 1] Array of activations\n",
    "        self.Z -- [num_neurons, 1] Array of linearly aggregated inputs\n",
    "        self.W -- [num_neurons, num_prev] Array of weights applied to previous layer's activations A.\n",
    "        self.b -- [num_neurons, 1] Array of biases\n",
    "        ''' \n",
    "        self.n = num_neurons\n",
    "        self.act = activation_function  \n",
    "        \n",
    "        self.A = np.zeros((num_neurons,1))\n",
    "        self.Z = np.zeros((num_neurons,1))\n",
    "        self.b = np.zeros((self.n,1))\n",
    "        \n",
    "        # He initialization.\n",
    "        if self.act == 'sigmoid':\n",
    "            self.W = np.random.randn(num_neurons, num_prev) * np.sqrt(2.0/num_prev)\n",
    "        \n",
    "    def forward(self, A_prev):\n",
    "        '''\n",
    "        Description:\n",
    "        Perform forward pass by computing Z, then A.\n",
    "        \n",
    "        Parameters:\n",
    "        A_prev -- [num_prev, m_samples] Array of activations from previous layer.\n",
    "        \n",
    "        Computes:\n",
    "        self.A -- [self.n, m_samples] Array of activations for this layer.\n",
    "        '''\n",
    "        self.Z = self.W @ A_prev + self.b\n",
    "        \n",
    "        if self.act == 'linear':\n",
    "            self.A = self.Z\n",
    "            \n",
    "        elif self.act == 'sigmoid':\n",
    "            self.A = 1 / (1 + np.exp(-self.Z))\n",
    "            \n",
    "        return self.A\n",
    "    \n",
    "    def backward(self, W_next, dZ_next, A_prev):\n",
    "        '''\n",
    "        Description:\n",
    "        Compute dA first, then dZ, then dW and db.\n",
    "        \n",
    "        Parameters:\n",
    "        W_next -- [num_next, self.n] Array of weights between this layer and next layer.\n",
    "        dZ_next -- [num_next, m_samples] Array of linear inputs to next layer.\n",
    "        A_prev -- [num_prev,m_samples] Array of activations from previous layer.\n",
    "        \n",
    "        Computes:\n",
    "        self.dA -- [self.n, m_samples] Gradient of cost w.r.t. activations.\n",
    "        self.dZ -- [self.n, m_samples] Gradient of cost w.r.t. linear inputs.\n",
    "        self.dW -- [self.n, num_prev] Gradient of cost w.r.t. weights.\n",
    "        self.db -- [self.n, 1] Gradient of cost w.r.t. biases.\n",
    "        '''        \n",
    "        m = self.A.shape[1]\n",
    "        self.dA = W_next.T @ dZ_next\n",
    "        \n",
    "        if self.act == 'linear':\n",
    "            self.dZ = self.dA\n",
    "            \n",
    "        elif self.act == 'sigmoid':\n",
    "            self.dZ = self.A * (1 - self.A) * self.dA\n",
    "            \n",
    "        self.dW = 1 / m * self.dZ @ A_prev.T\n",
    "        self.db = 1 / m * np.sum(self.dZ, axis=1, keepdims=True)\n",
    "        \n",
    "    def update(self, learning_rate):\n",
    "        self.W = self.W - learning_rate * self.dW\n",
    "        self.b = self.b - learning_rate * self.db\n",
    "    \n",
    "\n",
    "def costMSE(H, Y):\n",
    "    '''\n",
    "    Parameters:\n",
    "    H -- [1, m] Predicted values.\n",
    "    Y -- [1, m] Target values.\n",
    "    \n",
    "    Returns:\n",
    "    J -- (float) Mean squared error of dataset.\n",
    "    grad -- [1, m] Gradient of cost w.r.t. predicted values.\n",
    "    ''' \n",
    "    m = H.shape[1]\n",
    "    J = 1 / (2 * m) * (H - Y) @ (H - Y).T\n",
    "    grad = 1 / m * (H - Y)\n",
    "    return J, grad\n",
    "\n",
    "\n",
    "def costLogistic(H, Y):\n",
    "    '''\n",
    "    Parameters:\n",
    "    H -- [n, m] Predicted values in range (0, 1).\n",
    "    Y -- [n, m] Target values, either 0 or 1.\n",
    "    \n",
    "    Returns:\n",
    "    J -- [n, 1] Logistic cost of dataset.\n",
    "    grad -- [n, 1] Gradient of cost w.r.t. predicted values.\n",
    "    \n",
    "    '''\n",
    "    m = H.shape[1]\n",
    "    J = -1 / m * np.sum(Y * np.log(H) + (1 - Y) * np.log(1 - H), axis=1)\n",
    "    grad = 1 / m * np.sum(np.divide(-Y, H) + np.divide(1 - Y, 1 - H), axis=1, keepdims=True)\n",
    "    return J, grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 1: (3, 'sigmoid')\n",
      "Layer 2 \b: (2, 'sigmoid')\n",
      "Layer 3 \b: (1, 'sigmoid')\n"
     ]
    }
   ],
   "source": [
    "layout = ((3, 'sigmoid'),\n",
    "          (2, 'sigmoid'), \n",
    "          (1, 'sigmoid'),\n",
    "         ) \n",
    "\n",
    "network = buildNetwork(layout, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Checking Tool\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input\n",
      " [[0.5881308  0.89771373 0.89153073]\n",
      " [0.81583748 0.03588959 0.69175758]] \n",
      "\n",
      "Correct Answer\n",
      " [[0 1 1]\n",
      " [1 0 0]] \n",
      "\n",
      "Output\n",
      " [[0.36992296]\n",
      " [0.47231943]] \n",
      "\n",
      "Gradient\n",
      " [[0.06411603]\n",
      " [1.0185637 ]] \n",
      "\n",
      "Gradient Check\n",
      " [[0.06411603 0.06411603 0.06411603]\n",
      " [1.0185637  1.0185637  1.0185637 ]]\n"
     ]
    }
   ],
   "source": [
    "def function(H, Y):\n",
    "    m = H.shape[1]\n",
    "    return -1 / m * np.sum(Y * np.log(H) + (1 - Y) * np.log(1 - H), axis=1, keepdims=True)\n",
    "\n",
    "def gradient(H, Y):\n",
    "    m = H.shape[1]\n",
    "    return 1 / m * np.sum(np.divide(-Y, H) + np.divide(1 - Y, 1 - H), axis=1, keepdims=True)\n",
    "\n",
    "np.random.seed(20)\n",
    "Z = np.random.rand(2, 3)\n",
    "print('Input\\n', Z, '\\n')\n",
    "\n",
    "Y = np.array([[0, 1, 1],\n",
    "              [1, 0, 0]])\n",
    "print('Correct Answer\\n', Y, '\\n')\n",
    "\n",
    "A = function(Z, Y)\n",
    "print('Output\\n', A, '\\n')\n",
    "\n",
    "dAdZ = gradient(Z, Y)\n",
    "print('Gradient\\n', dAdZ, '\\n')\n",
    "\n",
    "eps = 1e-5\n",
    "dAdZ_check = np.zeros(Z.shape)\n",
    "\n",
    "for i in range(Z.shape[0]): \n",
    "    for j in range(Z.shape[1]):\n",
    "        cop = np.copy(Z)\n",
    "        bp[i,j] = cop[i,j] + eps\n",
    "        bn[i,j] = cop[i,j] - eps\n",
    "        plus = function(bp, Y)\n",
    "        minus = function(bn, Y)\n",
    "        dAdZ_check[i,j] = (plus[i] - minus[i]) / (2 * eps)\n",
    "print('Gradient Check\\n', dAdZ_check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
