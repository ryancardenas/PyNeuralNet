{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import pyneuralnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer(object):\n",
    "    def __init__(self, num_prev, num_neurons, activation_function='sigmoid'):\n",
    "        '''\n",
    "        Initialize weights randomly with He scaling.\n",
    "        \n",
    "        self.A -- [num_neurons, 1] Array of activations\n",
    "        self.Z -- [num_neurons, 1] Array of linearly aggregated inputs\n",
    "        self.W -- [num_neurons, num_prev] Array of weights applied to previous layer's activations A.\n",
    "        self.b -- [num_neurons, 1] Array of biases\n",
    "        ''' \n",
    "        self.n = num_neurons\n",
    "        self.act = activation_function  \n",
    "        \n",
    "        self.A = np.zeros((num_neurons,1))\n",
    "        self.Z = np.zeros((num_neurons,1))\n",
    "        self.b = np.zeros((self.n,1))\n",
    "        \n",
    "        # He initialization.\n",
    "        if self.act == 'sigmoid':\n",
    "            self.W = np.random.randn(num_neurons, num_prev) * np.sqrt(2.0/num_prev)\n",
    "        \n",
    "    def forward(self, A_prev):\n",
    "        '''\n",
    "        Description:\n",
    "        Perform forward pass by computing Z, then A.\n",
    "        \n",
    "        Parameters:\n",
    "        A_prev -- [num_prev, m_samples] Array of activations from previous layer.\n",
    "        \n",
    "        Computes:\n",
    "        self.A -- [self.n, m_samples] Array of activations for this layer.\n",
    "        '''\n",
    "        self.Z = self.W @ A_prev + self.b\n",
    "        \n",
    "        if self.act == 'linear':\n",
    "            self.A = self.Z\n",
    "            \n",
    "        elif self.act == 'sigmoid':\n",
    "            self.A = 1 / (1 + np.exp(-self.Z))\n",
    "            \n",
    "        return self.A\n",
    "    \n",
    "    def backward(self, W_next, dZ_next, A_prev):\n",
    "        '''\n",
    "        Description:\n",
    "        Compute dA first, then dZ, then dW and db.\n",
    "        \n",
    "        Parameters:\n",
    "        W_next -- [num_next, self.n] Array of weights between this layer and next layer.\n",
    "        dZ_next -- [num_next, m_samples] Array of linear inputs to next layer.\n",
    "        A_prev -- [num_prev,m_samples] Array of activations from previous layer.\n",
    "        \n",
    "        Computes:\n",
    "        self.dA -- [self.n, m_samples] Gradient of cost w.r.t. activations.\n",
    "        self.dZ -- [self.n, m_samples] Gradient of cost w.r.t. linear inputs.\n",
    "        self.dW -- [self.n, num_prev] Gradient of cost w.r.t. weights.\n",
    "        self.db -- [self.n, 1] Gradient of cost w.r.t. biases.\n",
    "        '''        \n",
    "        m = self.A.shape[1]\n",
    "        self.dA = W_next.T @ dZ_next\n",
    "        \n",
    "        if self.act == 'linear':\n",
    "            self.dZ = self.dA\n",
    "            \n",
    "        elif self.act == 'sigmoid':\n",
    "            self.dZ = self.A * (1 - self.A) * self.dA\n",
    "            \n",
    "        self.dW = 1 / m * self.dZ @ A_prev.T\n",
    "        self.db = 1 / m * np.sum(self.dZ, axis=1, keepdims=True)\n",
    "        \n",
    "    def update(self, learning_rate):\n",
    "        self.W = self.W - learning_rate * self.dW\n",
    "        self.b = self.b - learning_rate * self.db\n",
    "    \n",
    "\n",
    "def buildNetwork(layout, num_features):\n",
    "    '''\n",
    "    Arguments:\n",
    "    layout -- (num_layers, 2) Tuple where first column contains number of neurons in each layer \n",
    "                and second column contains activation functions for each layer.\n",
    "                \n",
    "    Returns:\n",
    "    network -- (num_layers,:) List of layers, each with number of neurons specified in 'network'.\n",
    "    '''\n",
    "    network = []\n",
    "    network.append(Layer(num_features, layout[0][0], layout[0][1]))\n",
    "    print('Layer 1:', layout[0])\n",
    "    \n",
    "    for i in range(1, len(layout)):\n",
    "        num_prev = layout[i-1][0]\n",
    "        num_neurons = layout[i][0]\n",
    "        activation_function = layout[i][1]\n",
    "        network.append(Layer(num_prev, num_neurons, activation_function))\n",
    "        print('Layer', i+1, '\\b:', layout[i])\n",
    "        \n",
    "    return network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 1: (3, 'sigmoid')\n",
      "Layer 2 \b: (2, 'sigmoid')\n",
      "Layer 3 \b: (1, 'sigmoid')\n"
     ]
    }
   ],
   "source": [
    "layout = ((3, 'sigmoid'),\n",
    "          (2, 'sigmoid'), \n",
    "          (1, 'sigmoid'),\n",
    "         ) \n",
    "\n",
    "network = buildNetwork(layout, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Checking Tool\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input\n",
      " [[ 3  5 -2]] \n",
      "\n",
      "Output\n",
      " [[0.95257413 0.99330715 0.11920292]] \n",
      "\n",
      "Gradient\n",
      " [[0.04517666 0.00664806 0.10499359]] \n",
      "\n",
      "Gradient Check\n",
      " [[0.04517666 0.00664806 0.10499359]]\n"
     ]
    }
   ],
   "source": [
    "def activation(Z):\n",
    "    return 1 / (1 + np.exp(-Z))\n",
    "\n",
    "def gradient(Z):\n",
    "    return A * (1 - A)\n",
    "\n",
    "Z = np.array([[3, 5, -2]])\n",
    "print('Input\\n', Z, '\\n')\n",
    "\n",
    "A = activation(Z)\n",
    "print('Output\\n', A, '\\n')\n",
    "\n",
    "dAdZ = gradient(Z)\n",
    "print('Gradient\\n', dAdZ, '\\n')\n",
    "\n",
    "eps = 1e-5\n",
    "plus = activation(Z + eps)\n",
    "minus = activation(Z - eps)\n",
    "dAdZ_check = (plus - minus) / (2 * eps)\n",
    "print('Gradient Check\\n', dAdZ_check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
